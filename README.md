

<h1 align="center"> Physical AI Lab </h1>

<div align="center">
<a href="https://pseudo-lab.com"><img src="https://img.shields.io/badge/PseudoLab-S11-3776AB" alt="PseudoLab"/></a>
<a href="https://discord.gg/EPurkHVtp2"><img src="https://img.shields.io/badge/Discord-BF40BF" alt="Discord Community"/></a>
<a href="https://github.com/Pseudo-Lab/PhyAI-Lab/stargazers"><img src="https://img.shields.io/github/stars/Pseudo-Lab/PhyAI-Lab" alt="Stars Badge"/></a>
<a href="https://github.com/Pseudo-Lab/PhyAI-Lab/network/members"><img src="https://img.shields.io/github/forks/Pseudo-Lab/PhyAI-Lab" alt="Forks Badge"/></a>
<a href="https://github.com/Pseudo-Lab/PhyAI-Lab/pulls"><img src="https://img.shields.io/github/issues-pr/Pseudo-Lab/PhyAI-Lab" alt="Pull Requests Badge"/></a>
<a href="https://github.com/Pseudo-Lab/PhyAI-Lab/issues"><img src="https://img.shields.io/github/issues/Pseudo-Lab/PhyAI-Lab" alt="Issues Badge"/></a>
<a href="https://github.com/Pseudo-Lab/PhyAI-Lab/graphs/contributors"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/Pseudo-Lab/PhyAI-Lab?color=2b9348"></a>
</div>
<br>



<!-- sheilds: https://shields.io/ -->
<!-- hits badge: https://hits.seeyoufarm.com/ -->

<p align="center">
  <img src="manipulation2.gif" alt="Demo" width="600"/>
</p>


🚀 **Physical AI Lab에 오신 것을 환영합니다!**\
저희는 **`Imitation Learning, Vision-Language-Action(VLA), Robot Reasoning/Planning`** 등 Physical AI의 최신 연구 흐름을 탐구합니다.
논문 리뷰와 토론을 통해 트렌드를 이해하고, **`로봇과 시뮬레이터(SO-ARM 101/Isaac Sim)`**
를 활용한 실험을 통해 한계점과 개선 방향을 분석합니다.

우리의 목표는 로봇이 실생활에 도입될 수 있는 길을 여는 것입니다.
Physical AI를 더 똑똑하고, 더 강력하며, 더 실용적으로 만들기 위한 여정에 함께하세요!

## 🌟 프로젝트 목표 (Project Vision)
- **논문 리뷰 목표**
    - 매주 한 편의 논문을 리뷰하며 Physical AI의 발전방향에 대해 이해
    - Open question에 대해 토론하며 비판적으로 논문을 해석하고 더욱 깊게 이해하는 능력 함양
- **프로젝트 목표**
    - 최신 연구들의 한계점을 파악하고 이를 개선하기 위한 인사이트 도출
    - 예시
        - 모델의 성능을 어떻게 평가할 수 있으며 적절한 데이터셋을 어떻게 curation 할 수 있을까?
        - VLA의 장기 메모리 부족 문제를 어떻게 해결할 수 있을까?
        - Teleoperation만으로는 LLM처럼 internet-scale의 데이터를 수집하기 어려움을 어떻게 해결할 것인가?
- **최종 성과물**
    - 논문 리뷰 아카이브
    - 실험 결과 및 인사이트 정리 → 추후 연구주제 선정         |


## 💻 주차별 활동 (Activity History)

| 날짜 | 선정 논문 | 발표자 | 발표자료
| -------- | -------- | ---- |------|
| 2025/09/09 | OT       |      |        |
| 2025/09/16 |  [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware](https://tonyzhaozh.github.io/aloha/)|김경준 |  [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/09/23 |  Magical Week | - | -
| 2025/09/30 |  미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/10/7 |  추석 | - |-
| 2025/10/14 |  미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/10/21 |  미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/10/28 |  Magical Week |- | -
| 2025/11/04 |  미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/11/11 | 미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/11/18 | 미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/11/25 |  미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/12/02 |  미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/12/09 |  미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/12/16 |  미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()
| 2025/12/23 |  미정 | 미정 | [![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/seong-yun-byeon-8183a8113/)]()

## 📚논문 리스트
- [π₀: A Vision-Language-Action Flow Model for General Robot Control](https://www.physicalintelligence.company/blog/pi0)


- [Gemini Robotics: Bringing AI into the Physical World](https://arxiv.org/abs/2503.20020)
- [FLARE: Robot Learning with Implicit World Modeling](https://research.nvidia.com/labs/gear/flare/)
- [Diffusion-VLA: Generalizable and Interpretable Robot Foundation Model via Self-Generated Reasoning](https://diffusion-vla.github.io/)
- [MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations](https://mimicgen.github.io/) 
- [GR00T N1: An Open Foundation Model for Generalist Humanoid Robots](https://arxiv.org/abs/2503.14734)
- [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/)
- [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://diffusion-policy.cs.columbia.edu/)
- [Say Can: Grounding Language in Robotic Affordances](https://say-can.github.io/)
- [OpenVLA:An Open-Source Vision-Language-Action Model](https://openvla.github.io/)
- [ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning](https://arxiv.org/abs/2507.16815)
- [MolmoAct: Action Reasoning Models that can Reason in Space](https://arxiv.org/abs/2508.07917)
- [CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models](https://cot-vla.github.io/)

### Survey Paper 
- Survey paper를 참고하여 발표논문을 선정하셔도 좋습니다.
- [A Survey on Vision-Language-Action Models for Embodied AI](https://arxiv.org/abs/2405.14093)
- [Vision Language Action Models in Robotic Manipulation: A Systematic Review](https://arxiv.org/abs/2507.10672)



## About Pseudo Lab 👋🏼</h2>

[Pseudo-Lab](https://pseudo-lab.com/) is a non-profit organization focused on advancing machine learning and AI technologies. Our core values of Sharing, Motivation, and Collaborative Joy drive us to create impactful open-source projects. With over 5k+ researchers, we are committed to advancing machine learning and AI technologies.

<h2>Contributors 😃</h2>
<a href="https://github.com/Pseudo-Lab/PhyAI-Lab/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=Pseudo-Lab/PhyAI-Lab" />
</a>
<br><br>

<h2>License 🗞</h2>

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT).

